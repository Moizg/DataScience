{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f3cd7f-d069-4089-b3cb-ff9fa94c0bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165ff1cd-1863-4824-8574-54fe7edd8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_df = pd.read_csv(r\"C:\\Users\\hp\\Downloads\\knn_dataset.csv\")\n",
    "nb_df = pd.read_csv(r\"C:\\Users\\hp\\Downloads\\naive_bayes_dataset.csv\")\n",
    "\n",
    "# Prepare data for KNN\n",
    "X_knn = knn_df[['Brightness', 'Saturation', 'Hue', 'Contrast']].values\n",
    "y_knn = knn_df['Class'].values\n",
    "\n",
    "# Encode labels for KNN\n",
    "le_knn = LabelEncoder()\n",
    "y_knn_encoded = le_knn.fit_transform(y_knn)\n",
    "\n",
    "# Split KNN data\n",
    "X_knn_train, X_knn_test, y_knn_train, y_knn_test = train_test_split(\n",
    "    X_knn, y_knn_encoded, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Prepare data for Naïve Bayes\n",
    "# Encode categorical features\n",
    "le_buy = LabelEncoder()\n",
    "le_win = LabelEncoder()\n",
    "le_length = LabelEncoder()\n",
    "le_chars = LabelEncoder()\n",
    "le_class = LabelEncoder()\n",
    "\n",
    "X_nb = np.column_stack([\n",
    "    le_buy.fit_transform(nb_df['Contains_Buy']),\n",
    "    le_win.fit_transform(nb_df['Contains_Win']),\n",
    "    le_length.fit_transform(nb_df['Email_Length']),\n",
    "    le_chars.fit_transform(nb_df['Special_Characters'])\n",
    "])\n",
    "y_nb = le_class.fit_transform(nb_df['Class'])\n",
    "\n",
    "# Split Naïve Bayes data\n",
    "X_nb_train, X_nb_test, y_nb_train, y_nb_test = train_test_split(\n",
    "    X_nb, y_nb, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f12964-eaf1-4a5c-b712-c92fc90edbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customKNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        \n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Store training data\"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for each input row\"\"\"\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        \"\"\"Predict class for a single row\"\"\"\n",
    "        # Compute distances between x and all examples in the training set\n",
    "        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        \n",
    "        # Sort by distance and return indices of the first k neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Extract the labels of the k nearest neighbor training samples\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        \n",
    "        # Return the most common class label\n",
    "        return max(set(k_nearest_labels), key=k_nearest_labels.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60dd6de6-fe15-497a-b9ab-38e92a6dd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_probabilities = {}\n",
    "        self.feature_probabilities = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Calculate class and feature probabilities\"\"\"\n",
    "        # Total number of samples\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Unique classes\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        # Calculate class probabilities\n",
    "        for cls in self.classes:\n",
    "            self.class_probabilities[cls] = np.sum(y == cls) / n_samples\n",
    "        \n",
    "        # Calculate feature probabilities for each class\n",
    "        self.feature_probabilities = {}\n",
    "        for cls in self.classes:\n",
    "            # Filter data for this class\n",
    "            X_cls = X[y == cls]\n",
    "            \n",
    "            # Calculate probability for each feature\n",
    "            cls_probs = {}\n",
    "            for feature in range(X.shape[1]):\n",
    "                unique_values = np.unique(X[:, feature])\n",
    "                feature_probs = {}\n",
    "                \n",
    "                for value in unique_values:\n",
    "                    # Laplace smoothing\n",
    "                    count = np.sum((X_cls[:, feature] == value))\n",
    "                    feature_probs[value] = (count + 1) / (len(X_cls) + len(unique_values))\n",
    "                \n",
    "                cls_probs[feature] = feature_probs\n",
    "            \n",
    "            self.feature_probabilities[cls] = cls_probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for each input sample\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for sample in X:\n",
    "            # Calculate posterior probability for each class\n",
    "            posteriors = {}\n",
    "            for cls in self.classes:\n",
    "                posterior = self.class_probabilities[cls]\n",
    "                \n",
    "                # Multiply by feature probabilities\n",
    "                for feature, value in enumerate(sample):\n",
    "                    # Use Laplace smoothing to handle unseen values\n",
    "                    posterior *= self.feature_probabilities[cls][feature].get(value, 1e-10)\n",
    "                \n",
    "                posteriors[cls] = posterior\n",
    "            \n",
    "            # Predict the class with highest posterior probability\n",
    "            predictions.append(max(posteriors, key=posteriors.get))\n",
    "        \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca8b1a1-812e-42aa-b74c-00dd728fd3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom KNN Classification\n",
    "custom_knn = customKNN(k=3)\n",
    "custom_knn.fit(X_knn_train, y_knn_train)\n",
    "custom_knn_pred = custom_knn.predict(X_knn_test)\n",
    "\n",
    "# Scikit-learn KNN Classification\n",
    "sklearn_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "sklearn_knn.fit(X_knn_train, y_knn_train)\n",
    "sklearn_knn_pred = sklearn_knn.predict(X_knn_test)\n",
    "\n",
    "# Custom Naïve Bayes Classification\n",
    "custom_nb = customNaiveBayes()\n",
    "custom_nb.fit(X_nb_train, y_nb_train)\n",
    "custom_nb_pred = custom_nb.predict(X_nb_test)\n",
    "\n",
    "# Scikit-learn Naïve Bayes Classification\n",
    "sklearn_nb = MultinomialNB()\n",
    "sklearn_nb.fit(X_nb_train, y_nb_train)\n",
    "sklearn_nb_pred = sklearn_nb.predict(X_nb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2eea83c-6aad-4659-8657-ed61981acfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classification Results:\n",
      "\n",
      "Custom KNN Accuracy: 0.6666666666666666\n",
      "Scikit-learn KNN Accuracy: 0.6666666666666666\n",
      "\n",
      "Naïve Bayes Classification Results:\n",
      "\n",
      "Custom Naïve Bayes Accuracy: 0.5333333333333333\n",
      "Scikit-learn Naïve Bayes Accuracy: 0.6\n",
      "\n",
      "Custom KNN Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Blue       0.33      0.25      0.29         4\n",
      "         Red       0.75      0.82      0.78        11\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.54      0.53      0.53        15\n",
      "weighted avg       0.64      0.67      0.65        15\n",
      "\n",
      "\n",
      "Scikit-learn KNN Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Blue       0.33      0.25      0.29         4\n",
      "         Red       0.75      0.82      0.78        11\n",
      "\n",
      "    accuracy                           0.67        15\n",
      "   macro avg       0.54      0.53      0.53        15\n",
      "weighted avg       0.64      0.67      0.65        15\n",
      "\n",
      "\n",
      "Custom Naïve Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Spam       0.50      0.86      0.63         7\n",
      "        Spam       0.67      0.25      0.36         8\n",
      "\n",
      "    accuracy                           0.53        15\n",
      "   macro avg       0.58      0.55      0.50        15\n",
      "weighted avg       0.59      0.53      0.49        15\n",
      "\n",
      "\n",
      "Scikit-learn Naïve Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Spam       0.54      1.00      0.70         7\n",
      "        Spam       1.00      0.25      0.40         8\n",
      "\n",
      "    accuracy                           0.60        15\n",
      "   macro avg       0.77      0.62      0.55        15\n",
      "weighted avg       0.78      0.60      0.54        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"KNN Classification Results:\")\n",
    "print(\"\\nCustom KNN Accuracy:\", accuracy_score(y_knn_test, custom_knn_pred))\n",
    "print(\"Scikit-learn KNN Accuracy:\", accuracy_score(y_knn_test, sklearn_knn_pred))\n",
    "\n",
    "print(\"\\nNaïve Bayes Classification Results:\")\n",
    "print(\"\\nCustom Naïve Bayes Accuracy:\", accuracy_score(y_nb_test, custom_nb_pred))\n",
    "print(\"Scikit-learn Naïve Bayes Accuracy:\", accuracy_score(y_nb_test, sklearn_nb_pred))\n",
    "\n",
    "# Detailed Classification Reports\n",
    "print(\"\\nCustom KNN Classification Report:\")\n",
    "print(classification_report(y_knn_test, custom_knn_pred, \n",
    "                            target_names=le_knn.classes_))\n",
    "\n",
    "print(\"\\nScikit-learn KNN Classification Report:\")\n",
    "print(classification_report(y_knn_test, sklearn_knn_pred, \n",
    "                            target_names=le_knn.classes_))\n",
    "\n",
    "print(\"\\nCustom Naïve Bayes Classification Report:\")\n",
    "print(classification_report(y_nb_test, custom_nb_pred, \n",
    "                            target_names=le_class.classes_))\n",
    "\n",
    "print(\"\\nScikit-learn Naïve Bayes Classification Report:\")\n",
    "print(classification_report(y_nb_test, sklearn_nb_pred, \n",
    "                            target_names=le_class.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c218de-e0f1-4ba4-94b3-3d5d55358181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
